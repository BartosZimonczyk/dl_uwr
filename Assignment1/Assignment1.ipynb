{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOq9DmD52l9ZwYdGlCpK8hb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/janchorowski/dl_uwr/blob/summer2020/Assignment1/Assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAiECkYEaZn1",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 1\n",
        "\n",
        "## Important notes\n",
        "**Submission deadline:**\n",
        "* **Thursday, 12.03.2020**\n",
        "\n",
        "**Points: 13 + 2bp**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bb0zN1GvapSt",
        "colab_type": "text"
      },
      "source": [
        "This assignment is meant to test your skills in course pre-requisites:  Scientific Python programming and  Machine Learning. If it is hard, I strongly advise you to drop the course.\n",
        "\n",
        "Please use GitHubâ€™s [pull requests](https://help.github.com/en/github/collaborating-with-issues-and-pull-requests/about-pull-requests) and issues to send corrections!\n",
        "\n",
        "You can solve the assignment in any system you like, but we encourage you to try out [Google Colab](https://colab.research.google.com/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIBf-IF_ahTI",
        "colab_type": "text"
      },
      "source": [
        "## Assignment text\n",
        "1. **[1p]** Download data competition from a Kaggle competition on sentiment prediction from [[https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/data](https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/data)].  Keep only full sentences, i.e. for each `SenteceId` keep only the entry with the lowest `PhraseId`.  Use first 7000 sentences as a `train set` and the remaining 1529 sentences as the `test set`. \n",
        "\n",
        "2. **[1p]** Prepare the data for logistic regression:\n",
        "\tMap the sentiment scores $0,1,2,3,4$ to a probability of the sentence being by setting $p(\\textrm{positive}) = \\textrm{sentiment}/4$.\n",
        "\tBuild a dictionary of most frequent 20000 words.\n",
        "\n",
        "3. **[3p]** Treat each document as a bag of words. e.g. if the vocabulary is \n",
        "\t```\n",
        "\t0: the\n",
        "\t1: good\n",
        "\t2: movie\n",
        "\t3: is\n",
        "\t4: not\n",
        "\t5: a\n",
        "\t6: funny\n",
        "\t```\n",
        "\tThen the encodings can be:\n",
        "\t```\n",
        "\tgood:                           [0,1,0,0,0,0,0]\n",
        "\tnot good:                       [0,1,0,0,1,0,0] \n",
        "\tthe movie is not a funny movie: [1,0,2,1,1,1,1]\n",
        "\t```\n",
        "    Train a logistic regression model to predict the sentiment. Compute the correlation between the predicted probabilities and the sentiment. Record the most positive and negative words.\n",
        "    Please note that in this model each word gets its sentiment parameter $S_w$ and the score for a sentence is \n",
        "    $$\\text{score}(\\text{sentence}) = \\sum_{w\\text{ in sentence}}S_w$$\n",
        "\n",
        "4. **[3p]** Now prepare an encoding in which negation flips the sign of the following words. For instance for our vocabulary the encodings become:\n",
        "\t```\n",
        "\tgood:                           [0,1,0,0,0,0,0]\n",
        "\tnot good:                       [0,-1,0,0,1,0,0]\n",
        "\tnot not good:                   [0,1,0,0,0,0,0]\n",
        "\tthe movie is not a funny movie: [1,0,0,1,1,-1,-1]\n",
        "\t```\n",
        "\tFor best results, you will probably need to construct a list of negative words.\n",
        "\t\n",
        "\tAgain train a logistic regression classifier and compare the results to the Bag of Words approach.\n",
        "\t\n",
        "\tPlease note that this model still maintains a single parameter for each word, but now the sentence score is\n",
        "\t$$\\text{score}(\\text{sentence}) = \\sum_{w\\text{ in sentence}}-1^{\\text{count of negations preceeding }w}S_w$$\n",
        "\n",
        "5. **[5p]** Now also consider emphasizing words such as `very`. They can boost (multiply by a constant >1) the following words.\n",
        "\tImplement learning the modifying multiplier for negation and for emphasis. One way to do this is to introduce a model which has:\n",
        "\t- two modifiers, $N$ for negation and $E$ for emphasis\n",
        "\t- a sentiment score $S_w$ for each word \n",
        "And score each sentence as:\n",
        "$$\\text{score}(\\text{sentence}) = \\sum_{w\\text{ in sentence}}N^{\\text{\\#negs prec. }w}E^{\\text{\\#emphs prec. }w}S_w$$\n",
        "\n",
        "You will need to implement a custom logistic regression model to support it.\n",
        "\n",
        "6. **[2pb]** Propose, implement, and evaluate an extension to the above model.\n"
      ]
    }
  ]
}